{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Incorporating CNNs\n",
    "\n",
    "* Learning Objective: In this problem, you will learn how to deeply understand how Convolutional Neural Networks work by implementing one.\n",
    "* Provided Code: We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* TODOs: you will implement a Convolutional Layer and a MaxPooling Layer to improve on your classification results in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.mlp.fully_conn import *\n",
    "from lib.mlp.layer_utils import *\n",
    "from lib.mlp.train import *\n",
    "from lib.cnn.layer_utils import *\n",
    "from lib.cnn.cnn_models import *\n",
    "from lib.datasets import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-100 with 20 superclasses)\n",
    "\n",
    "In this homework, we will be classifying images from the CIFAR-100 dataset into the 20 superclasses. More information about the CIFAR-100 dataset and the 20 superclasses can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Download the CIFAR-100 data files [here](https://drive.google.com/drive/folders/1imXxTnpkMbWEe41pkAGNt_JMTXECDSaW?usp=share_link), and save the `.mat` files to the `data/cifar100` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: data_train Shape: (40000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_train Shape: (40000,), <class 'numpy.ndarray'>\n",
      "Name: data_val Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_val Shape: (10000,), <class 'numpy.ndarray'>\n",
      "Name: data_test Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_test Shape: (10000,), <class 'numpy.ndarray'>\n",
      "label_names: ['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "Name: mean_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n",
      "Name: std_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR100_data('data/cifar100/')\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print (\"Name: {} Shape: {}, {}\".format(k, v.shape, type(v)))\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n",
    "label_names = data['label_names']\n",
    "mean_image = data['mean_image'][0]\n",
    "std_image = data['std_image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: large_omnivores_and_herbivores\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "image_data = data['data_train'][idx]\n",
    "image_data = ((image_data*std_image + mean_image) * 255).astype(np.int32)\n",
    "plt.imshow(image_data)\n",
    "label = label_names[data['labels_train'][idx]]\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "We will use convolutional neural networks to try to improve on the results from Problem 1. Convolutional layers make the assumption that local pixels are more important for prediction than far-away pixels. This allows us to form networks that are robust to small changes in positioning in images.\n",
    "\n",
    "### Convolutional Layer Output size calculation [2pts]\n",
    "\n",
    "As you have learned, two important parameters of a convolutional layer are its stride and padding. To warm up, we will need to calculate the output size of a convolutional layer given its stride and padding. To do this, open the `lib/cnn/layer_utils.py` file and fill out the TODO section in the `get_output_size` function in the ConvLayer2D class. \n",
    "\n",
    "Implement your function so that it returns the correct size as indicated by the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received [32, 16, 16, 16] and expected [32, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "input_image = np.zeros([32, 28, 28, 3]) # a stack of 32 28 by 28 rgb images\n",
    "\n",
    "in_channels = input_image.shape[-1] #must agree with the last dimension of the input image\n",
    "k_size = 4 \n",
    "n_filt = 16\n",
    "\n",
    "conv_layer = ConvLayer2D(in_channels, k_size, n_filt, stride=2, padding=3)\n",
    "output_size = conv_layer.get_output_size(input_image.shape) \n",
    "\n",
    "print(\"Received {} and expected [32, 16, 16, 16]\".format(output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE LATER\n",
    "# input_image_shape = [32,28,28,3] # (batch_size, in_height, in_width, in_channels)\n",
    "# in_height = input_image_shape[1]\n",
    "# in_width = input_image_shape[2]\n",
    "# in_channel = 3 # RGB\n",
    "\n",
    "# k_size # kernel size\n",
    "# n_filt # Filter size\n",
    "# stride = 2 # Stride\n",
    "# padding = 3 # Padding\n",
    "\n",
    "# #((i-k+2p)/s)  + 1\n",
    "# dim = ((in_height-k_size+(2*padding))//stride) + 1\n",
    "# print(dim)\n",
    "\n",
    "# batch_size, in_height, in_width, in_channels = input_image.shape\n",
    "# print(batch_size, in_height, in_width, in_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer Forward Pass [5pts]\n",
    "\n",
    "Now, we will implement the forward pass of a convolutional layer. Fill in the TODO block in the `forward` function of the ConvLayer2D class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 4, 4, 2), Expected output shape: (1, 4, 4, 2)\n",
      "Difference:  5.110565335399418e-08\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=1*8*8*1).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "in_channels, k_size, n_filt = 1, 5, 2\n",
    "\n",
    "weight_size = k_size*k_size*in_channels*n_filt\n",
    "bias_size = n_filt\n",
    "\n",
    "\n",
    "\n",
    "single_conv = ConvLayer2D(in_channels, k_size, n_filt, stride=1, padding=0, name=\"conv_test\")\n",
    "\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(k_size, k_size, in_channels, n_filt)\n",
    "b = np.linspace(-0.3, 0.3, num=bias_size)\n",
    "\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "out = single_conv.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 4, 4, 2)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[-0.03874312, 0.57000324],\n",
    "   [-0.03955296, 0.57081309],\n",
    "   [-0.04036281, 0.57162293],\n",
    "   [-0.04117266, 0.57243278]],\n",
    "\n",
    "  [[-0.0452219, 0.57648202],\n",
    "   [-0.04603175, 0.57729187],\n",
    "   [-0.04684159, 0.57810172],\n",
    "   [-0.04765144, 0.57891156]],\n",
    "\n",
    "  [[-0.05170068, 0.5829608 ],\n",
    "   [-0.05251053, 0.58377065],\n",
    "   [-0.05332038, 0.5845805 ],\n",
    "   [-0.05413022, 0.58539035]],\n",
    "\n",
    "  [[-0.05817946, 0.58943959],\n",
    "   [-0.05898931, 0.59024943],\n",
    "   [-0.05979916, 0.59105928],\n",
    "   [-0.06060901, 0.59186913]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  [[[[-0.03874312  0.57000324]\n",
      "   [-0.03955296  0.57081309]\n",
      "   [-0.04036281  0.57162293]\n",
      "   [-0.04117266  0.57243278]]\n",
      "\n",
      "  [[-0.0452219   0.57648202]\n",
      "   [-0.04603175  0.57729187]\n",
      "   [-0.04684159  0.57810172]\n",
      "   [-0.04765144  0.57891156]]\n",
      "\n",
      "  [[-0.05170068  0.5829608 ]\n",
      "   [-0.05251053  0.58377065]\n",
      "   [-0.05332038  0.5845805 ]\n",
      "   [-0.05413022  0.58539035]]\n",
      "\n",
      "  [[-0.05817946  0.58943959]\n",
      "   [-0.05898931  0.59024943]\n",
      "   [-0.05979916  0.59105928]\n",
      "   [-0.06060901  0.59186913]]]] [[[[-0.03874312  0.57000324]\n",
      "   [-0.03955296  0.57081309]\n",
      "   [-0.04036281  0.57162293]\n",
      "   [-0.04117266  0.57243278]]\n",
      "\n",
      "  [[-0.0452219   0.57648202]\n",
      "   [-0.04603175  0.57729187]\n",
      "   [-0.04684159  0.57810172]\n",
      "   [-0.04765144  0.57891156]]\n",
      "\n",
      "  [[-0.05170068  0.5829608 ]\n",
      "   [-0.05251053  0.58377065]\n",
      "   [-0.05332038  0.5845805 ]\n",
      "   [-0.05413022  0.58539035]]\n",
      "\n",
      "  [[-0.05817946  0.58943959]\n",
      "   [-0.05898931  0.59024943]\n",
      "   [-0.05979916  0.59105928]\n",
      "   [-0.06060901  0.59186913]]]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Difference: \",out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Layer Backward [5pts]\n",
    "\n",
    "Now complete the backward pass of a convolutional layer. Fill in the TODO block in the `backward` function of the ConvLayer2D class. Check you results with this code and expect differences of less than 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  9.513594161788744e-08\n",
      "dw Error:  2.2862882501622255e-08\n",
      "db Error:  1.606687904173235e-10\n",
      "dimg Shape:  (15, 8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the conv backward function\n",
    "img = np.random.randn(15, 8, 8, 3)\n",
    "w = np.random.randn(4, 4, 3, 12)\n",
    "b = np.random.randn(12)\n",
    "dout = np.random.randn(15, 4, 4, 12)\n",
    "\n",
    "single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "single_conv.params[single_conv.w_name] = w\n",
    "single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: single_conv.forward(img), img, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_conv.forward(img), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_conv.forward(img), b, dout)\n",
    "\n",
    "out = single_conv.forward(img)\n",
    "\n",
    "dimg = single_conv.backward(dout)\n",
    "dw = single_conv.grads[single_conv.w_name]\n",
    "db = single_conv.grads[single_conv.b_name]\n",
    "\n",
    "# The error should be around 1e-6\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The errors should be around 1e-8\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4, 4, 12) (15, 8, 8, 3) (4, 4, 3, 12)\n",
      "(16, 3, 1)\n",
      "[[[-1.04482060e+01 -1.85591838e+00 -1.84941329e+01]\n",
      "  [-1.67400035e+01 -2.16687759e+01  1.87492827e+01]\n",
      "  [-1.09581107e+01 -4.41262209e+00  8.23694044e+00]\n",
      "  [-3.83553854e-01  1.68703655e+01  3.63457498e+00]]\n",
      "\n",
      " [[-1.15715814e+01  1.68253322e+01  1.31266428e+01]\n",
      "  [-1.80128820e+01  2.24394603e+01  4.30523299e+00]\n",
      "  [-3.00395004e+01 -9.34577049e+00 -1.58941394e+01]\n",
      "  [-5.67583445e+00 -1.38931334e-02 -1.76391724e+01]]\n",
      "\n",
      " [[ 9.40303056e+00 -2.10969009e+01  1.16399278e+01]\n",
      "  [ 1.39960083e+01  4.45978034e+00  6.87115159e-01]\n",
      "  [-1.73170809e+01  1.43220448e+01 -5.63666796e-01]\n",
      "  [ 1.93473756e+00 -3.62349710e+00 -1.69565807e+01]]\n",
      "\n",
      " [[ 2.83248754e+00  1.93147789e+01  3.73097661e+00]\n",
      "  [-2.09622113e+00 -1.34009927e+01 -4.75065928e+00]\n",
      "  [-1.21056738e+01 -2.09323934e+01 -2.20502298e+00]\n",
      "  [ 7.35240809e+00 -1.67391150e+01  5.15007714e+00]]] \n",
      "************\n",
      " [[[-1.04482060e+01 -1.85591838e+00 -1.84941329e+01]\n",
      "  [-1.67400035e+01 -2.16687759e+01  1.87492827e+01]\n",
      "  [-1.09581107e+01 -4.41262209e+00  8.23694044e+00]\n",
      "  [-3.83553853e-01  1.68703655e+01  3.63457498e+00]]\n",
      "\n",
      " [[-1.15715814e+01  1.68253322e+01  1.31266428e+01]\n",
      "  [-1.80128820e+01  2.24394603e+01  4.30523299e+00]\n",
      "  [-3.00395004e+01 -9.34577049e+00 -1.58941394e+01]\n",
      "  [-5.67583445e+00 -1.38931328e-02 -1.76391724e+01]]\n",
      "\n",
      " [[ 9.40303056e+00 -2.10969009e+01  1.16399278e+01]\n",
      "  [ 1.39960083e+01  4.45978034e+00  6.87115160e-01]\n",
      "  [-1.73170809e+01  1.43220448e+01 -5.63666795e-01]\n",
      "  [ 1.93473756e+00 -3.62349710e+00 -1.69565807e+01]]\n",
      "\n",
      " [[ 2.83248754e+00  1.93147789e+01  3.73097661e+00]\n",
      "  [-2.09622113e+00 -1.34009927e+01 -4.75065928e+00]\n",
      "  [-1.21056738e+01 -2.09323934e+01 -2.20502298e+00]\n",
      "  [ 7.35240809e+00 -1.67391150e+01  5.15007714e+00]]]\n",
      "[[[[ 29.54834445  -2.07616516  -3.97258773  12.24300654  -1.67964873\n",
      "     -2.15727719  -9.3005181   11.27563026 -17.20901674  -3.68259504\n",
      "    -19.49273513  10.94499621]]]] \n",
      "************\n",
      " [ 29.54834445  -2.07616516  -3.97258773  12.24300654  -1.67964873\n",
      "  -2.15727719  -9.3005181   11.27563026 -17.20901674  -3.68259504\n",
      " -19.49273514  10.94499621]\n"
     ]
    }
   ],
   "source": [
    "print(dout.shape, img.shape, w.shape)\n",
    "print(w[:,:,:,0].reshape(-1,w.shape[2],1).shape)\n",
    "print(dw[:,:,:,0],'\\n************\\n',dw_num[:,:,:,0])\n",
    "print(db,'\\n************\\n',db_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "\n",
    "# # Test the conv backward function\n",
    "# img = np.random.randn(15, 8, 8, 3)\n",
    "# w = np.random.randn(4, 4, 3, 12)\n",
    "# b = np.random.randn(12)\n",
    "# dout = np.random.randn(15, 4, 4, 12)\n",
    "\n",
    "# single_conv = ConvLayer2D(input_channels=3, kernel_size=4, number_filters=12, stride=2, padding=1, name=\"conv_test\")\n",
    "# single_conv.params[single_conv.w_name] = w\n",
    "# single_conv.params[single_conv.b_name] = b\n",
    "\n",
    "# dimg_num = eval_numerical_gradient_array(lambda x: single_conv.forward(img), img, dout)\n",
    "# dw_num = eval_numerical_gradient_array(lambda w: single_conv.forward(img), w, dout)\n",
    "# db_num = eval_numerical_gradient_array(lambda b: single_conv.forward(img), b, dout)\n",
    "\n",
    "# out = single_conv.forward(img)\n",
    "\n",
    "# dimg = single_conv.backward(dout)\n",
    "# dw = single_conv.grads[single_conv.w_name]\n",
    "# db = single_conv.grads[single_conv.b_name]\n",
    "\n",
    "# # The error should be around 1e-6\n",
    "# print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# # The errors should be around 1e-8\n",
    "# print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "# print(\"db Error: \", rel_error(db_num, db))\n",
    "# # # The shapes should be same\n",
    "# print(\"dimg Shape: \", dimg.shape, img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE LATER\n",
    "# for i in range(m):\n",
    "#     for ii in range(0,input_height + (2*P)-K+1,S):\n",
    "#         for jj in range(0,input_width+ (2*P)-K+1,S):\n",
    "#             for c in range(C):\n",
    "#                 for f in range(N):\n",
    "#                     dL_dW = dprev[batch,:,:,f].reshape(-1,1) \n",
    "#                     x = img_padded[batch, ii:ii + K, jj : jj+K, c].reshape(-1,1)\n",
    "#                     dw[ii//S, jj//S,c, f] = np.dot(dL_dW.T,x)\n",
    "\n",
    "#                     dL_dX = dprev[batch,:,:,f].reshape(-1,1) \n",
    "#                     w_re = w[:,:, c, f].reshape(-1,1)\n",
    "#                     dx_padded[batch, ii//S, jj//S, c] += np.dot(dL_dX.T,w_re)\n",
    "\n",
    "# dx_depadded = dx_padded[:, P:-P,P:-P, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without paddinng or stride\n",
    "# for batch in range(batch_size):\n",
    "#     for ii in range(0,input_height-K):\n",
    "#         for jj in range(0,input_width-K):\n",
    "#             for f in range(N):\n",
    "#                 dL_dO = dprev[batch,:,:,f].reshape(-1,1) # [K*K*C,f]\n",
    "#                 for c in range(C):\n",
    "#                     x = img[batch, ii:ii + K, jj : jj+K, c].reshape(-1,1)\n",
    "# #                     print(img[batch, ii:ii + K, jj : jj+K, c],'\\n\\n', dprev[batch,:,:,f])\n",
    "# #                     print(x,'\\n\\n', dL_dO)\n",
    "#                     dw[ii, jj,c, f] = np.dot(dL_dO.T,x)\n",
    "# #                     print(dw[ii, jj,c, f],dw_num[ii,jj,c,f])\n",
    "#                     # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "# #                     da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "# #                     dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "# #                     db[:,:,:,c] += dZ[i, h, w, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling Layer\n",
    "Now we will implement maxpooling layers, which can help to reduce the image size while preserving the overall structure of the image.\n",
    "\n",
    "### Forward Pass max pooling [5pts]\n",
    "Fill out the TODO block in the `forward` function of the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received output shape: (1, 3, 3, 1), Expected output shape: (1, 3, 3, 1)\n",
      "Difference:  1.8750000280978013e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the convolutional forward function\n",
    "input_image = np.linspace(-0.1, 0.4, num=64).reshape([1, 8, 8, 1]) # a single 8 by 8 grayscale image\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "out = maxpool.forward(input_image)\n",
    "\n",
    "print(\"Received output shape: {}, Expected output shape: (1, 3, 3, 1)\".format(out.shape))\n",
    "\n",
    "correct_out = np.array([[\n",
    "   [[0.11428571],\n",
    "   [0.13015873],\n",
    "   [0.14603175]],\n",
    "\n",
    "  [[0.24126984],\n",
    "   [0.25714286],\n",
    "   [0.27301587]],\n",
    "\n",
    "  [[0.36825397],\n",
    "   [0.38412698],\n",
    "   [0.4       ]]]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.11428571]\n",
      "   [0.13015873]\n",
      "   [0.14603175]]\n",
      "\n",
      "  [[0.24126984]\n",
      "   [0.25714286]\n",
      "   [0.27301587]]\n",
      "\n",
      "  [[0.36825397]\n",
      "   [0.38412698]\n",
      "   [0.4       ]]]] \n",
      "****\n",
      " [[[[0.11428571]\n",
      "   [0.13015873]\n",
      "   [0.14603175]]\n",
      "\n",
      "  [[0.24126984]\n",
      "   [0.25714286]\n",
      "   [0.27301587]]\n",
      "\n",
      "  [[0.36825397]\n",
      "   [0.38412698]\n",
      "   [0.4       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "print(out,'\\n****\\n', correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass Max pooling [5pts]\n",
    "Fill out the `backward` function in the MaxPoolingLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimg Error:  3.2758974079763358e-12\n",
      "dimg Shape:  (15, 8, 8, 3) (15, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "img = np.random.randn(15, 8, 8, 3)\n",
    "\n",
    "dout = np.random.randn(15, 3, 3, 3)\n",
    "\n",
    "maxpool= MaxPoolingLayer(pool_size=4, stride=2, name=\"maxpool_test\")\n",
    "\n",
    "dimg_num = eval_numerical_gradient_array(lambda x: maxpool.forward(img), img, dout)\n",
    "\n",
    "out = maxpool.forward(img)\n",
    "dimg = maxpool.backward(dout)\n",
    "\n",
    "# The error should be around 1e-8\n",
    "print(\"dimg Error: \", rel_error(dimg_num, dimg))\n",
    "# The shapes should be same\n",
    "print(\"dimg Shape: \", dimg.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3, 3, 3)\n",
      "[[[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.52991749  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.38111518  0.          0.\n",
      "    0.          0.06401859  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.06802654 -1.039279\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.18745934  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -1.17620809  0.        ]\n",
      "  [ 0.          0.          0.          1.62694144  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -1.10292062  0.          0.          0.\n",
      "   -0.46243128  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-2.28035552  0.          0.          0.          0.\n",
      "    0.          0.03575213  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.77144831\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.91973488  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          1.83193616  0.\n",
      "    0.          0.          1.41980047]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.16728232  0.        ]\n",
      "  [ 0.          0.          0.          1.88686193  0.\n",
      "    0.          0.          0.        ]\n",
      "  [-0.99579587  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.70616268  0.          0.\n",
      "    0.          0.1949686   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.47810683  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.06357303  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.05243696\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.42460573  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -0.04764124  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.70608624  0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-1.33901821  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.26954     0.          0.\n",
      "    0.          0.          0.85777032]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          1.60064456  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.96266348\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.48462795  0.          0.\n",
      "   -0.40808792  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          1.12960507  0.\n",
      "    0.         -0.66845818  0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    1.39352774  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -1.85110918\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -2.6078694   0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.27080932]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          1.01506163  0.         -1.70112483  0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.01603718  0.          3.24680227\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.34014148  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.69611426  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          2.60479627  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -0.64562126  0.         -0.1106487 ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.80337362  0.          0.\n",
      "    0.          0.          0.19685187]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -0.37760777  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.3937882   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.83066438  0.          0.          0.67338728\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.62175811]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.29150708\n",
      "    0.          0.          0.        ]\n",
      "  [-0.47540321  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.99797876]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.76567196  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          1.37916985  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.40817914  0.          0.\n",
      "    0.          0.66638299  0.        ]]\n",
      "\n",
      " [[ 0.         -0.09833628  0.          0.          0.18785062\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.88877627  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.37242675  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.88450262  0.          0.\n",
      "    0.4445852   0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.0504458   0.93492739  0.          0.\n",
      "    0.         -1.35945573  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.31475727]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          1.10281139\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.50467769  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -1.42886325  0.          0.        ]\n",
      "  [-1.41348055  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 1.38895711  0.          0.          0.          0.\n",
      "    0.71124581  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          3.04725611  0.         -0.25294689\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.18779203  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]] \n",
      "****\n",
      " [[[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.52991749  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.38111518  0.          0.\n",
      "    0.          0.06401859  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.06802654 -1.039279\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    1.18745934  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -1.17620809  0.        ]\n",
      "  [ 0.          0.          0.          1.62694144  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -1.10292062  0.          0.          0.\n",
      "   -0.46243128  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-2.28035552  0.          0.          0.          0.\n",
      "    0.          0.03575213  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.77144831\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.91973488  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          1.83193616  0.\n",
      "    0.          0.          1.41980047]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.16728232  0.        ]\n",
      "  [ 0.          0.          0.          1.88686193  0.\n",
      "    0.          0.          0.        ]\n",
      "  [-0.99579587  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -1.70616268  0.          0.\n",
      "    0.          0.1949686   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.47810683  0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.06357303  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.05243696\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -0.42460573  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.         -0.04764124  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.70608624  0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [-1.33901821  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.26954     0.          0.\n",
      "    0.          0.          0.85777032]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          1.60064456  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.96266348\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.48462795  0.          0.\n",
      "   -0.40808792  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          1.12960507  0.\n",
      "    0.         -0.66845818  0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    1.39352774  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -1.85110918\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         -2.6078694   0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.27080932]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          1.01506163  0.         -1.70112483  0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          1.01603718  0.          3.24680227\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.34014148  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.69611426  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          2.60479627  0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -0.64562126  0.         -0.1106487 ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.80337362  0.          0.\n",
      "    0.          0.          0.19685187]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         -0.37760777  0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.3937882   0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.83066438  0.          0.          0.67338728\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          1.62175811]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.         -0.29150708\n",
      "    0.          0.          0.        ]\n",
      "  [-0.47540321  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.99797876]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.         -2.76567196  0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          1.37916985  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.40817914  0.          0.\n",
      "    0.          0.66638299  0.        ]]\n",
      "\n",
      " [[ 0.         -0.09833628  0.          0.          0.18785062\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.88877627  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.37242675  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.88450262  0.          0.\n",
      "    0.4445852   0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.0504458   0.93492739  0.          0.\n",
      "    0.         -1.35945573  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.         -0.31475727]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          1.10281139\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.50467769  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "   -1.42886325  0.          0.        ]\n",
      "  [-1.41348055  0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 1.38895711  0.          0.          0.          0.\n",
      "    0.71124581  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          3.04725611  0.         -0.25294689\n",
      "    0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.         -0.18779203  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "print(out.shape)\n",
    "print(dimg_num[:,:,:,0],'\\n****\\n', dimg[:,:,:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Small Convolutional Neural Network [3pts]\n",
    "Please find the `TestCNN` class in `lib/cnn/cnn_models.py`.\n",
    "Again you only need to complete few lines of code in the TODO block.\n",
    "Please design a Convolutional --> Maxpool --> flatten --> fc network where the shapes of parameters match the given shapes.\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively.\n",
    "Here you only modify the param_name part, the _w, and _b are automatically assigned during network setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Passed!\n",
      "Testing test-time forward pass ... \n",
      "Passed!\n",
      "Testing the loss ...\n",
      "Passed!\n",
      "Testing the gradients (error should be no larger than 1e-6) ...\n",
      "conv1_b relative error: 1.01e-09\n",
      "conv1_w relative error: 1.05e-09\n",
      "fc1_b relative error: 3.65e-10\n",
      "fc1_w relative error: 3.95e-07\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TestCNN()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "B, H, W, iC = 4, 8, 8, 3 #batch, height, width, in_channels\n",
    "k = 3 #kernel size\n",
    "oC, Hi, O = 3, 27, 5 # out channels, Hidden Layer input, Output size\n",
    "std = 0.02\n",
    "x = np.random.randn(B,H,W,iC)\n",
    "y = np.random.randint(O, size=B)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"conv1_w\").std() - std)\n",
    "b1 = model.net.get_params(\"conv1_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"fc1_w\").std() - std)\n",
    "b2 = model.net.get_params(\"fc1_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=k*k*iC*oC).reshape(k,k,iC,oC)\n",
    "w2 = np.linspace(-0.2, 0.2, num=Hi*O).reshape(Hi, O)\n",
    "b1 = np.linspace(-0.6, 0.2, num=oC)\n",
    "b2 = np.linspace(-0.9, 0.1, num=O)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "model.net.assign(\"conv1_w\", w1)\n",
    "model.net.assign(\"conv1_b\", b1)\n",
    "model.net.assign(\"fc1_w\", w2)\n",
    "model.net.assign(\"fc1_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=B*H*W*iC).reshape(B,H,W,iC)\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[-13.85107294, -11.52845818,  -9.20584342,  -6.88322866,  -4.5606139 ],\n",
    " [-11.44514171, -10.21200524 , -8.97886878 , -7.74573231 , -6.51259584],\n",
    " [ -9.03921048,  -8.89555231 , -8.75189413 , -8.60823596,  -8.46457778],\n",
    " [ -6.63327925 , -7.57909937 , -8.52491949 , -9.4707396 , -10.41655972]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 2, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 4.56046848799693\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network [25pts]\n",
    "In this section, we defined a `SmallConvolutionalNetwork` class for you to fill in the TODO block in `lib/cnn/cnn_models.py`.\n",
    "\n",
    "Here please design a network with at most two convolutions and two maxpooling layers (you may use less).\n",
    "You can adjust the parameters for any layer, and include layers other than those listed above that you have implemented (such as fully-connected layers and non-linearities).\n",
    "You are also free to select any optimizer you have implemented (with any learning rate).\n",
    "\n",
    "You will train your network on CIFAR-100 20-way superclass classification.\n",
    "Try to find a combination that is able to achieve 40% validation accuracy.\n",
    "\n",
    "Since the CNN takes significantly longer to train than the fully connected network, it is suggested to start off with fewer filters in your Conv layers and fewer intermediate fully-connected layers so as to get faster initial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40000, 32, 32, 3)\n",
      "Flattened data input size: 3072\n",
      "Number of data classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data_dict[\"data_train\"][0].shape)\n",
    "print(\"Flattened data input size:\", np.prod(data[\"data_train\"].shape[1:]))\n",
    "print(\"Number of data classes:\", max(data['labels_train']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "But got (100, 2700) and 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m reg_lambda \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39m#############################################################################\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#                             END OF YOUR CODE                              #\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#############################################################################\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m results \u001b[39m=\u001b[39m train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n\u001b[0;32m     27\u001b[0m                     lr_decay, lr_decay_every, show_every\u001b[39m=\u001b[39;49m\u001b[39m4000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, regularization\u001b[39m=\u001b[39;49mregularization, reg_lambda\u001b[39m=\u001b[39;49mreg_lambda)\n\u001b[0;32m     28\u001b[0m opt_params, loss_hist, train_acc_hist, val_acc_hist \u001b[39m=\u001b[39m results\n",
      "File \u001b[1;32mc:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\personal_repo\\CSCI-566-Deep-Learning-and-its-Applications\\assignment1\\lib\\mlp\\train.py:161\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(data, model, loss_func, optimizer, batch_size, max_epochs, lr_decay, lr_decay_every, show_every, verbose, regularization, reg_lambda)\u001b[0m\n\u001b[0;32m    149\u001b[0m data_batch, labels_batch \u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39mget_batch()\n\u001b[0;32m    151\u001b[0m \u001b[39m#############################################################################\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39m# TODO: Update the parameters by a forward pass for the network, a backward #\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m# pass to the network, and make a step for the optimizer.                   #\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[39m#Initiate forward pass and calculate output and gradients\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(data_batch, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    163\u001b[0m \u001b[39m#Calling loss function : Forward and backward pass for loss (passed above)\u001b[39;00m\n\u001b[0;32m    164\u001b[0m loss \u001b[39m=\u001b[39m loss_func\u001b[39m.\u001b[39mforward(output, labels_batch)\n",
      "File \u001b[1;32mc:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\personal_repo\\CSCI-566-Deep-Learning-and-its-Applications\\assignment1\\lib\\mlp\\fully_conn.py:20\u001b[0m, in \u001b[0;36mModule.forward\u001b[1;34m(self, feat, is_training, seed)\u001b[0m\n\u001b[0;32m     18\u001b[0m         output \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mforward(output, is_training, seed)\n\u001b[0;32m     19\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m         output \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(output)\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mgather_params()\n\u001b[0;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\personal_repo\\CSCI-566-Deep-Learning-and-its-Applications\\assignment1\\lib\\mlp\\layer_utils.py:182\u001b[0m, in \u001b[0;36mfc.forward\u001b[1;34m(self, feat)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, feat):\n\u001b[0;32m    181\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(feat\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m feat\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim, \\\n\u001b[0;32m    183\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(feat\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim)\n\u001b[0;32m    184\u001b[0m     \u001b[39m#############################################################################\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[39m# TODO: Implement the forward pass of a single fully connected layer.       #\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39m# Store the results in the variable output provided above.                  #\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39m#############################################################################\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: But got (100, 2700) and 3072"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallConvolutionalNetwork()\n",
    "loss_f = cross_entropy()\n",
    "\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "# You may only adjust the hyperparameters within this block                 #\n",
    "#############################################################################\n",
    "optimizer = Adam(model.net, 1e-3)\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "lr_decay = .999\n",
    "lr_decay_every = 10\n",
    "regularization = \"none\"\n",
    "reg_lambda = 0.01\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n",
    "                    lr_decay, lr_decay_every, show_every=4000, verbose=True, regularization=regularization, reg_lambda=reg_lambda)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_cnn_train.txt', 'wb') as f:\n",
    "    pickle.dump(results_cnn_train,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to generate the training plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100]  # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layers [5pts]\n",
    "\n",
    "An interesting finding from early research in convolutional networks was that the learned convolutions resembled filters used for things like edge detection. Complete the code below to visualize the filters in the first convolutional layer of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_array = None\n",
    "nrows, ncols = None, None\n",
    "\n",
    "###################################################\n",
    "# TODO: read the weights in the convolutional     #\n",
    "# layer and reshape them to a grid of images to   #\n",
    "# view with matplotlib.                           #\n",
    "###################################################\n",
    "\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "plt.imshow(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question: Comment below on what kinds of filters you see. Include your response in your submission [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Credit: Analysis on Trained Model [5pts]\n",
    "\n",
    "For extra credit, you can perform some additional analysis of your trained model. Some suggested analyses are:\n",
    "1. Plot the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of your model's predictions on the test set. Look for trends to see which classes are frequently misclassified as other classes (e.g. are the two vehicle superclasses frequently confused with each other?).\n",
    "2. Implement [BatchNorm](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739) and analyze how the models train with and without BatchNorm.\n",
    "3. Introduce some small noise in the labels, and investigate how that affects training and validation accuracy.\n",
    "\n",
    "You are free to choose any analysis question of interest to you. We will not be providing any starter code for the extra credit. Include your extra-credit analysis as the final section of your report pdf, titled \"Extra Credit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_2_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for CNN training\n",
    "2. Visualization of convolutional filters\n",
    "3. Answers to inline questions about convolutional filters\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbconvert import HTMLExporter\n",
    "import codecs\n",
    "import nbformat\n",
    "\n",
    "notebook_name = 'Problem_2.ipynb'\n",
    "output_file_name = 'output_prob_2.html'\n",
    "\n",
    "exporter = HTMLExporter()\n",
    "output_notebook = nbformat.read(notebook_name, as_version=4)\n",
    "\n",
    "output, resources = exporter.from_notebook_node(output_notebook)\n",
    "codecs.open(output_file_name, 'w', encoding='utf-8').write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cc545a8e4c6f3672ca7df11aed171cf02f21b0bdd4cab8a79d349af5c398917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
