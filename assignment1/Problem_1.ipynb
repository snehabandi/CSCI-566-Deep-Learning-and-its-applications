{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Basics of Neural Networks\n",
    "* <b>Learning Objective:</b> In this problem, you are asked to implement a basic multi-layer fully connected neural network from scratch, including forward and backward passes of certain essential layers, to perform an image classification task on the CIFAR100 dataset. You need to implement essential functions in different indicated python files under directory `lib`.\n",
    "* <b>Provided Code:</b> We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* <b>TODOs:</b> You are asked to implement the forward passes and backward passes for standard layers and loss functions, various widely-used optimizers, and part of the training procedure. And finally we want you to train a network from scratch on your own. Also, there are inline questions you need to answer. See `README.md` to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.mlp.fully_conn import *\n",
    "from lib.mlp.layer_utils import *\n",
    "from lib.datasets import *\n",
    "from lib.mlp.train import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-100 with 20 superclasses)\n",
    "\n",
    "In this homework, we will be classifying images from the CIFAR-100 dataset into the 20 superclasses. More information about the CIFAR-100 dataset and the 20 superclasses can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Download the CIFAR-100 data files [here](https://drive.google.com/drive/folders/1imXxTnpkMbWEe41pkAGNt_JMTXECDSaW?usp=share_link), and save the `.mat` files to the `data/cifar100` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: data_train Shape: (40000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_train Shape: (40000,), <class 'numpy.ndarray'>\n",
      "Name: data_val Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_val Shape: (10000,), <class 'numpy.ndarray'>\n",
      "Name: data_test Shape: (10000, 32, 32, 3), <class 'numpy.ndarray'>\n",
      "Name: labels_test Shape: (10000,), <class 'numpy.ndarray'>\n",
      "label_names: ['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']\n",
      "Name: mean_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n",
      "Name: std_image Shape: (1, 1, 1, 3), <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR100_data('data/cifar100/')\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print (\"Name: {} Shape: {}, {}\".format(k, v.shape, type(v)))\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n",
    "label_names = data['label_names']\n",
    "mean_image = data['mean_image'][0]\n",
    "std_image = data['std_image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Standard Layers\n",
    "You will now implement all the following standard layers commonly seen in a fully connected neural network (aka multi-layer perceptron, MLP). Please refer to the file `lib/mlp/layer_utils.py`. Take a look at each class skeleton, and we will walk you through the network layer by layer. We provide results of some examples we pre-computed for you for checking the forward pass, and also the gradient checking for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Forward [2pt]\n",
    "In the class skeleton `flatten` and `fc` in `lib/mlp/layer_utils.py`, please complete the forward pass in function `forward`. The input to the `fc` layer may not be of dimension (batch size, features size), it could be an image or any higher dimensional data. We want to convert the input to have a shape of (batch size, features size). Make sure that you handle this dimensionality issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  4.02601593296122e-09\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc forward function\n",
    "input_bz = 3 # batch size\n",
    "input_dim = (7, 6, 4)\n",
    "output_dim = 4\n",
    "\n",
    "input_size = input_bz * np.prod(input_dim)\n",
    "weight_size = output_dim * np.prod(input_dim)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "single_fc = fc(np.prod(input_dim), output_dim, init_scale=0.02, name=\"fc_test\")\n",
    "\n",
    "x = np.linspace(-0.1, 0.4, num=input_size).reshape(input_bz, *input_dim)\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(np.prod(input_dim), output_dim)\n",
    "b = np.linspace(-0.3, 0.3, num=output_dim)\n",
    "\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "out = single_fc.forward(flatten_layer.forward(x))\n",
    "\n",
    "correct_out = np.array([[0.63910291, 0.83740057, 1.03569824, 1.23399591],\n",
    "                        [0.61401587, 0.82903823, 1.04406058, 1.25908294],\n",
    "                        [0.58892884, 0.82067589, 1.05242293, 1.28416997]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-8\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE\n",
    "# print(x.shape,w.shape,b.shape, out.shape)\n",
    "# p = flatten_layer.forward(x)\n",
    "# q = np.vectorize(lambda x : x)\n",
    "# print(q(p).shape)\n",
    "# print(np.dot(w.T,q(p)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Backward [2pt]\n",
    "Please complete the function `backward` as the backward pass of the `flatten` and `fc` layers. Follow the instructions in the comments to store gradients into the predefined dictionaries in the attributes of the class. Parameters of the layer are also stored in the predefined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx Error:  3.6400031043911565e-09\n",
      "dw Error:  4.0274568221082673e-10\n",
      "db Error:  6.747580410242694e-11\n",
      "dinp Shape:  (15, 2, 2, 3) (15, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc backward function\n",
    "inp = np.random.randn(15, 2, 2, 3)\n",
    "w = np.random.randn(12, 15)\n",
    "b = np.random.randn(15)\n",
    "dout = np.random.randn(15, 15)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "x = flatten_layer.forward(inp)\n",
    "single_fc = fc(np.prod(x.shape[1:]), 15, init_scale=5e-2, name=\"fc_test\")\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: single_fc.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_fc.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_fc.forward(x), b, dout)\n",
    "\n",
    "out = single_fc.forward(x)\n",
    "dx = single_fc.backward(dout)\n",
    "dw = single_fc.grads[single_fc.w_name]\n",
    "db = single_fc.grads[single_fc.b_name]\n",
    "dinp = flatten_layer.backward(dx)\n",
    "\n",
    "# The error should be around 1e-9\n",
    "print(\"dx Error: \", rel_error(dx_num, dx))\n",
    "# The errors should be around 1e-10\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# The shapes should be same\n",
    "print(\"dinp Shape: \", dinp.shape, inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #REMOVE LATER\n",
    "# x, dprev = out, dout\n",
    "\n",
    "# dx = np.dot(dprev, w.T)\n",
    "# dw = np.dot(out.T, dprev)\n",
    "# db = np.sum(dprev, axis=0)\n",
    "\n",
    "# print(x.shape, dprev.shape)\n",
    "# print(x.reshape(dprev.shape[0],-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeLU Forward [2pt]\n",
    "In the class skeleton `gelu` in `lib/mlp/layer_utils.py`, please complete the `forward` pass.\n",
    "\n",
    "GeLU is a smooth version of ReLU and it's used in pre-training LLMs such as GPT-3 and BERT. \n",
    "\n",
    "$$\n",
    "\\mathrm{GeLU}(x) = x \\Phi(x) \\approx 0.5x(1+\\tanh(\\sqrt{2/\\pi}(x+0.044715x^3)))\n",
    "$$\n",
    "\n",
    "Where $\\Phi(x)$ is the CDF for standard Gaussian random variables.  You should use the approximate version to compute forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  1.8037541876132445e-08\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the leaky_relu forward function\n",
    "x = np.linspace(-1.5, 1.5, num=12).reshape(3, 4)\n",
    "gelu_f = gelu(name=\"gelu_f\")\n",
    " \n",
    "out = gelu_f.forward(x)\n",
    "correct_out = np.array([[-0.10042842, -0.13504766, -0.16231757, -0.1689214 ],\n",
    "                        [-0.13960493, -0.06078651,  0.07557713,  0.26948598],\n",
    "                        [ 0.51289678,  0.79222788,  1.09222506,  1.39957158]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10042842, -0.13504766, -0.16231757, -0.1689214 ],\n",
       "       [-0.13960493, -0.06078651,  0.07557713,  0.26948598],\n",
       "       [ 0.51289678,  0.79222788,  1.09222506,  1.39957158]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REMOVE LATER\n",
    "m = 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) *(x + 0.044715 * np.power(x, 3))))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeLU Backward [2pt]\n",
    "Please complete the `backward` pass of the class `gelu`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx Error:  9.155618377606901e-10\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the relu backward function\n",
    "x = np.random.randn(15, 15)\n",
    "dout = np.random.randn(*x.shape)\n",
    "gelu_b = gelu(name=\"gelu_b\")\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: gelu_b.forward(x), x, dout)\n",
    "\n",
    "out = gelu_b.forward(x)\n",
    "dx = gelu_b.backward(dout)\n",
    "\n",
    "# The error should not be larger than 1e-4, since we are using an approximate version of GeLU activation. \n",
    "print (\"dx Error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15) (15, 15) (15, 15) (15, 15)\n",
      "(15, 15) (15, 15) (15, 15)\n",
      "[ 0.86681009  0.84764584 -1.26944143  0.07689572 -0.50802157  2.74309875\n",
      "  0.18373144 -0.39511351 -0.54848451  1.23922502 -0.67192374  0.68527143\n",
      " -0.01967683  0.09851876  0.55084601] [ 0.98383514  0.02166846 -1.41944657  0.08639382 -0.52593372  2.92639079\n",
      "  0.18003989 -0.34849314 -0.37014062  1.41189533  0.20627917 -0.14799454\n",
      "  0.00540911  0.11179262  0.55281413]\n"
     ]
    }
   ],
   "source": [
    "# #REMOVE LATER\n",
    "x, dprev = out, dout\n",
    "\n",
    "a = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "b = np.cosh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))\n",
    "binv = 1/b\n",
    "c = x*(1 + 3*0.044715 * np.power(x, 2))\n",
    "derivative =  (np.power(binv,2) * c)/np.sqrt(2*np.pi) + a\n",
    "dfeat = (dprev * derivative)\n",
    "\n",
    "# print (\"dx Error: \", rel_error(dx_num, dx))\n",
    "print(a.shape, b.shape, binv.shape, c.shape)\n",
    "print(dprev.shape,derivative.shape, dx_num.shape)\n",
    "print(dfeat[0],dx_num[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Forward [2pt]\n",
    "In the class `dropout` in `lib/mlp/layer_utils.py`, please complete the `forward` pass.  \n",
    "Remember that the dropout is **only applied during training phase**, you should pay attention to this while implementing the function.\n",
    "##### Important Note1: The probability argument input to the function is the \"keep probability\": probability that each activation is kept.\n",
    "##### Important Note2: If the keep_prob is set to 1, make it as no dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Dropout Keep Prob =  0\n",
      "Mean of input:  4.978586231409696\n",
      "Mean of output during training time:  4.978586231409696\n",
      "Mean of output during testing time:  4.978586231409696\n",
      "Fraction of output set to zero during training time:  0.0\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout Keep Prob =  0.25\n",
      "Mean of input:  4.978586231409696\n",
      "Mean of output during training time:  5.151336364345049\n",
      "Mean of output during testing time:  4.978586231409696\n",
      "Fraction of output set to zero during training time:  0.7417\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout Keep Prob =  0.5\n",
      "Mean of input:  4.978586231409696\n",
      "Mean of output during training time:  5.058131211362833\n",
      "Mean of output during testing time:  4.978586231409696\n",
      "Fraction of output set to zero during training time:  0.4943\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout Keep Prob =  0.75\n",
      "Mean of input:  4.978586231409696\n",
      "Mean of output during training time:  4.96299957630882\n",
      "Mean of output during testing time:  4.978586231409696\n",
      "Fraction of output set to zero during training time:  0.2517\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout Keep Prob =  1\n",
      "Mean of input:  4.978586231409696\n",
      "Mean of output during training time:  4.978586231409696\n",
      "Mean of output during testing time:  4.978586231409696\n",
      "Fraction of output set to zero during training time:  0.0\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(100, 100) + 5.0\n",
    "\n",
    "print (\"----------------------------------------------------------------\")\n",
    "for p in [0, 0.25, 0.50, 0.75, 1]:\n",
    "    dropout_f = dropout(keep_prob=p)\n",
    "    out = dropout_f.forward(x, True)\n",
    "    out_test = dropout_f.forward(x, False)\n",
    "\n",
    "    # Mean of output should be similar to mean of input\n",
    "    # Means of output during training time and testing time should be similar\n",
    "    print (\"Dropout Keep Prob = \", p)\n",
    "    print (\"Mean of input: \", x.mean())\n",
    "    print (\"Mean of output during training time: \", out.mean())\n",
    "    print (\"Mean of output during testing time: \", out_test.mean())\n",
    "    print (\"Fraction of output set to zero during training time: \", (out == 0).mean())\n",
    "    print (\"Fraction of output set to zero during testing time: \", (out_test == 0).mean())\n",
    "    print (\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "[[5.94522247 6.13889376 3.17146817 6.36423181 4.65396518]\n",
      " [4.7004534  3.9598056  3.95571259 6.36466367 4.1743487 ]\n",
      " [5.86792025 4.49060395 5.02294132 3.33339406 4.06446344]\n",
      " [5.69599211 5.4445139  7.18015538 6.71550318 5.17237827]\n",
      " [5.35364654 3.31807357 3.78163036 5.08283091 5.65189758]] \n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "[[0.74259653 0.6299931  0.03071739 0.96375146 0.1975359 ]\n",
      " [0.96803117 0.49985212 0.89098489 0.98885445 0.75948342]\n",
      " [0.58433138 0.34348413 0.49509552 0.74076183 0.95253597]\n",
      " [0.08634581 0.32352674 0.60484209 0.65706592 0.45168174]\n",
      " [0.41203125 0.66525686 0.29469086 0.77531258 0.43372377]] \n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "[[False False  True False  True]\n",
      " [False False False False False]\n",
      " [False False False False False]\n",
      " [ True False False False False]\n",
      " [False False False False False]] \n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "[[0. 0. 4. 0. 4.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [4. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.          0.         12.68587266  0.         18.61586071]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [22.78396846  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "Dropout Keep Prob =  0.25\n",
      "Mean of input:  5.024188407044462\n",
      "Mean of output during training time:  2.1634280734722626\n",
      "Mean of output during testing time:  4.978586231409696\n"
     ]
    }
   ],
   "source": [
    "# Remove Later ->tester code\n",
    "x = np.random.randn(5, 5) + 5.0\n",
    "\n",
    "p, is_training = 0.25, True\n",
    "rng = np.random.RandomState(seed=None)\n",
    "\n",
    "print(x.shape)\n",
    "print(x,'\\n\\n***\\n\\n')\n",
    "see = rng.rand(*x.shape)\n",
    "print(see,'\\n\\n***\\n\\n')\n",
    "print(see<p,'\\n\\n***\\n\\n')\n",
    "print((see<p)/p)\n",
    "\n",
    "if is_training == True and p>0 and p<1:          \n",
    "    mask = (see < p) / p\n",
    "    out = x * mask\n",
    "    print(out)\n",
    "#Test Phase : mask = None, kept = input\n",
    "else:    \n",
    "    out_test = x\n",
    "    print(out_test)\n",
    "print (\"Dropout Keep Prob = \", p)\n",
    "print (\"Mean of input: \", x.mean())\n",
    "print (\"Mean of output during training time: \", out.mean())\n",
    "print (\"Mean of output during testing time: \", out_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Backward [2pt]\n",
    "Please complete the `backward` pass. Again remember that the dropout is only applied during training phase, handle this in the backward pass as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx relative error:  3.003115711868424e-11\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(5, 5) + 5\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "keep_prob = 0.75\n",
    "dropout_b = dropout(keep_prob, seed=100)\n",
    "out = dropout_b.forward(x, True, seed=1)\n",
    "dx = dropout_b.backward(dout)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_b.forward(xx, True, seed=1), x, dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print ('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "[[ 0.16521457 -0.06928663 -0.42532866  0.30371946 -0.72734045]\n",
      " [ 1.86017785  0.07330684 -0.69510586 -1.2752528  -0.18587725]\n",
      " [ 0.85059223 -1.27687724  0.76691414  1.62254228  0.07479543]\n",
      " [ 0.59958216 -0.17342926 -0.61274522  0.6708616  -0.42399826]\n",
      " [-1.18572511  0.12151672  0.8648055   0.08190942  0.39673626]] \n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "[[1.33333333 1.33333333 1.33333333 1.33333333 1.33333333]\n",
      " [1.33333333 1.33333333 1.33333333 1.33333333 1.33333333]\n",
      " [1.33333333 1.33333333 1.33333333 0.         1.33333333]\n",
      " [1.33333333 1.33333333 1.33333333 1.33333333 1.33333333]\n",
      " [0.         0.         1.33333333 1.33333333 0.        ]] \n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "[[ 0.2202861  -0.09238218 -0.56710488  0.40495929 -0.96978727]\n",
      " [ 2.48023713  0.09774245 -0.92680781 -1.70033706 -0.24783633]\n",
      " [ 1.13412297 -1.70250299  1.02255219  0.          0.09972724]\n",
      " [ 0.79944288 -0.23123901 -0.81699362  0.89448214 -0.56533102]\n",
      " [-0.          0.          1.153074    0.10921256  0.        ]]\n",
      "dx relative error:  3.003116679315532e-11\n"
     ]
    }
   ],
   "source": [
    "# Remove Later ->Dropout Backward tester code\n",
    "x = np.random.randn(5, 5) + 5\n",
    "dout = np.random.randn(*x.shape)\n",
    "keep_prob = 0.75\n",
    "dropout_b = dropout(keep_prob, seed=100)\n",
    "out = dropout_b.forward(x, True, seed=1)\n",
    "\n",
    "p,dprev, is_training = keep_prob,dout, True\n",
    "rng = np.random.RandomState(seed=None)\n",
    "\n",
    "mask = out/x\n",
    "\n",
    "print(dprev.shape)\n",
    "print(dprev,'\\n\\n***\\n\\n')\n",
    "print(mask,'\\n\\n***\\n\\n')\n",
    "\n",
    "if is_training == True and p>0 and p<1:\n",
    "    dfeat = dprev * mask\n",
    "else:\n",
    "    dfeat = dprev\n",
    "\n",
    "dx = dfeat\n",
    "print(dx)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_b.forward(xx, True, seed=1), x, dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print ('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cascaded layers: FC + GeLU [2pt]\n",
    "Please find the `TestFCGeLU` function in `lib/mlp/fully_conn.py`. <br />\n",
    "You only need to complete a few lines of code in the TODO block. <br />\n",
    "Please design an `Flatten -> FC -> GeLU` network where the parameters of them match the given x, w, and b. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  8.355578090311344e-10\n",
      "dw error:  2.0294001439670866e-10\n",
      "db error:  3.0802021224842033e-11\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(3, 5, 3)  # the input features\n",
    "w = np.random.randn(15, 5)   # the weight of fc layer\n",
    "b = np.random.randn(5)       # the bias of fc layer\n",
    "dout = np.random.randn(3, 5) # the gradients to the output, notice the shape\n",
    "\n",
    "tiny_net = TestFCGeLU()\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "tiny_net.net.assign(\"fc_w\", w)\n",
    "tiny_net.net.assign(\"fc_b\", b)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "out = tiny_net.forward(x)\n",
    "dx = tiny_net.backward(dout)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "dw = tiny_net.net.get_grads(\"fc_w\")\n",
    "db = tiny_net.net.get_grads(\"fc_b\")\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tiny_net.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: tiny_net.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: tiny_net.forward(x), b, dout)\n",
    "\n",
    "# The errors should not be larger than 1e-7\n",
    "print (\"dx error: \", rel_error(dx_num, dx))\n",
    "print (\"dw error: \", rel_error(dw_num, dw))\n",
    "print (\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Function and Loss Layer [2pt]\n",
    "In the `lib/mlp/layer_utils.py`, please first complete the function `softmax`, which will be used in the function `cross_entropy`. Then, implement `corss_entropy` using `softmax`.\n",
    "Please refer to the lecture slides of the mathematical expressions of the cross entropy loss function, and complete its forward pass and backward pass. You should also take care of `size_average` on whether or not to divide by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:  1.791817114324856\n",
      "dx error:  6.180929681183048e-09\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "num_classes, num_inputs = 6, 100\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "test_loss = cross_entropy()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: test_loss.forward(x, y), x, verbose=False)\n",
    "\n",
    "loss = test_loss.forward(x, y)\n",
    "dx = test_loss.backward()\n",
    "\n",
    "# Test softmax_loss function. Loss should be around 1.792\n",
    "# and dx error should be at the scale of 1e-8 (or smaller)\n",
    "print (\"Cross Entropy Loss: \", loss)\n",
    "print (\"dx error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Small Fully Connected Network [2pt]\n",
    "Please find the `SmallFullyConnectedNetwork` function in `lib/mlp/fully_conn.py`. <br />\n",
    "Again you only need to complete few lines of code in the TODO block. <br />\n",
    "Please design an `FC --> GeLU --> FC` network where the shapes of parameters match the given shapes. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. <br />\n",
    "Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Passed!\n",
      "Testing test-time forward pass ... \n",
      "Passed!\n",
      "Testing the loss ...\n",
      "Passed!\n",
      "Testing the gradients (error should be no larger than 1e-6) ...\n",
      "fc1_b relative error: 5.94e-09\n",
      "fc1_w relative error: 1.06e-08\n",
      "fc2_b relative error: 4.01e-10\n",
      "fc2_w relative error: 2.50e-08\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallFullyConnectedNetwork()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "N, D, = 4, 4  # N: batch size, D: input dimension\n",
    "H, C  = 30, 7 # H: hidden dimension, C: output dimension\n",
    "std = 0.02\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"fc1_w\").std() - std)\n",
    "b1 = model.net.get_params(\"fc1_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"fc2_w\").std() - std)\n",
    "b2 = model.net.get_params(\"fc2_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "w2 = np.linspace(-0.2, 0.2, num=H*C).reshape(H, C)\n",
    "b1 = np.linspace(-0.6, 0.2, num=H)\n",
    "b2 = np.linspace(-0.9, 0.1, num=C)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "model.net.assign(\"fc1_w\", w1)\n",
    "model.net.assign(\"fc1_b\", b1)\n",
    "model.net.assign(\"fc2_w\", w2)\n",
    "model.net.assign(\"fc2_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[-2.33881897, -1.92174121, -1.50466344, -1.08758567, -0.6705079, -0.25343013,  0.16364763],\n",
    "                             [-1.57214916, -1.1857013 , -0.79925345, -0.41280559, -0.02635774, 0.36009011,  0.74653797],\n",
    "                             [-0.80178618, -0.44604469, -0.0903032 ,  0.26543829,  0.62117977, 0.97692126,  1.33266275],\n",
    "                             [-0.00331319,  0.32124836,  0.64580991,  0.97037146,  1.29493301, 1.61949456,  1.94405611]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 5, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 2.4248995879903195\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Fully Connected Network regularized with Dropout [2pt]\n",
    "Please find the `DropoutNet` function in `fully_conn.py` under `lib/mlp` directory. <br />\n",
    "For this part you don't need to design a new network, just simply run the following test code. <br />\n",
    "If something goes wrong, you might want to double check your dropout implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout p = 0\n",
      "Error of gradients should be around or less than 1e-3\n",
      "fc1_b relative error: 9.824168615238225e-08\n",
      "fc1_w relative error: 4.70635574719171e-06\n",
      "fc2_b relative error: 1.1334029457022126e-08\n",
      "fc2_w relative error: 3.167223146070301e-05\n",
      "fc3_b relative error: 2.0518174276833617e-10\n",
      "fc3_w relative error: 2.720304740415546e-06\n",
      "\n",
      "Dropout p = 0.25\n",
      "Error of gradients should be around or less than 1e-3\n",
      "fc1_b relative error: 1.8949591872374903e-07\n",
      "fc1_w relative error: 3.4287142973862415e-06\n",
      "fc2_b relative error: 1.6435766065275814e-07\n",
      "fc2_w relative error: 4.52072681731756e-05\n",
      "fc3_b relative error: 2.1474160887299336e-10\n",
      "fc3_w relative error: 7.9382903586546e-07\n",
      "\n",
      "Dropout p = 0.5\n",
      "Error of gradients should be around or less than 1e-3\n",
      "fc1_b relative error: 3.613140832277011e-07\n",
      "fc1_w relative error: 4.60442874211552e-07\n",
      "fc2_b relative error: 1.7902142291542414e-08\n",
      "fc2_w relative error: 7.923786508162106e-06\n",
      "fc3_b relative error: 3.285178756580047e-10\n",
      "fc3_w relative error: 1.103448593805289e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D, C = 3, 15, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for keep_prob in [0, 0.25, 0.5]:\n",
    "    np.random.seed(seed=seed)\n",
    "    print (\"Dropout p =\", keep_prob)\n",
    "    model = DropoutNet(keep_prob=keep_prob, seed=seed)\n",
    "    loss_func = cross_entropy()\n",
    "    output = model.forward(X, True, seed=seed)\n",
    "    loss = loss_func.forward(output, y)\n",
    "    dLoss = loss_func.backward()\n",
    "    dX = model.backward(dLoss)\n",
    "    grads = model.net.grads\n",
    "\n",
    "    print (\"Error of gradients should be around or less than 1e-3\")\n",
    "    for name in sorted(grads):\n",
    "        if name not in model.net.params.keys():\n",
    "            continue\n",
    "        f = lambda _: loss_func.forward(model.forward(X, True, seed=seed), y)\n",
    "        grad_num = eval_numerical_gradient(f, model.net.params[name], verbose=False, h=1e-5)\n",
    "        print (\"{} relative error: {}\".format(name, rel_error(grad_num, grads[name])))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "In this section, we defined a `TinyNet` class for you to fill in the TODO block in `lib/mlp/fully_conn.py`.\n",
    "* Here please design a two layer fully connected network with Leaky ReLU activation (`Flatten --> FC --> GeLU --> FC`).\n",
    "* You can adjust the number of hidden neurons, batch_size, epochs, and learning rate decay parameters.\n",
    "* Please read the `lib/train.py` carefully and complete the TODO blocks in the `train_net` function first. Codes in \"Test a Small Fully Connected Network\" can be helpful.\n",
    "* Implement SGD in `lib/optim.py`, you will be asked to complete weight decay and Adam in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40000, 32, 32, 3)\n",
      "Flattened data input size: 3072\n",
      "Number of data classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data[\"data_train\"].shape)\n",
    "print(\"Flattened data input size:\", np.prod(data[\"data_train\"].shape[1:]))\n",
    "print(\"Number of data classes:\", max(data['labels_train']) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the network to achieve at least 30% validation accuracy [5pt]\n",
    "You may only adjust the hyperparameters inside the TODO block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 1/400 [00:02<18:28,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 2000) Average loss: 2.997345440512727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [19:24<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 5) Training Accuracy: 0.29, Validation Accuracy: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [21:56<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2 / 5) Training Accuracy: 0.33, Validation Accuracy: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [19:26<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3 / 5) Training Accuracy: 0.43, Validation Accuracy: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [17:51<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4 / 5) Training Accuracy: 0.3, Validation Accuracy: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [18:08<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 5) Training Accuracy: 0.45, Validation Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TinyNet()\n",
    "loss_f = cross_entropy()\n",
    "optimizer = SGD(model.net, 0.1)\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "#############################################################################\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "lr_decay = 0.99\n",
    "lr_decay_every = 100\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs,\n",
    "                    lr_decay, lr_decay_every, show_every=10000, verbose=True)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(opt_params, loss_hist, train_acc_hist, val_acc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1_w', 'fc1_b', 'fc2_w', 'fc2_b'])\n"
     ]
    }
   ],
   "source": [
    "# Take a look at what names of params were stored\n",
    "print (opt_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Params: fc1_w Shape: (3072, 48)\n",
      "Loading Params: fc1_b Shape: (48,)\n",
      "Loading Params: fc2_w Shape: (48, 20)\n",
      "Loading Params: fc2_b Shape: (20,)\n",
      "Validation Accuracy: 31.52%\n",
      "Testing Accuracy: 31.55%\n"
     ]
    }
   ],
   "source": [
    "# Demo: How to load the parameters to a newly defined network\n",
    "model = TinyNet()\n",
    "model.net.load(opt_params)\n",
    "val_acc = compute_acc(model, data[\"data_val\"], data[\"labels_val\"])\n",
    "print (\"Validation Accuracy: {}%\".format(val_acc*100))\n",
    "test_acc = compute_acc(model, data[\"data_test\"], data[\"labels_test\"])\n",
    "print (\"Testing Accuracy: {}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100] # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Optimizers and Regularization Techniques\n",
    "There are several more advanced optimizers than vanilla SGD, and there are many regularization tricks. You'll implement them in this section.\n",
    "Please complete the TODOs in the `lib/optim.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Weight Decay [2pt]\n",
    "The update rule of SGD plus weigh decay is as shown below:  \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_t - \\eta \\nabla_{\\theta}J(\\theta_t) - \\lambda \\theta_t\n",
    "\\end{align*}\n",
    "Update the `SGD()` function in `lib/optim.py`, and also incorporate weight decay options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following errors should be around or less than 1e-6\n",
      "updated_w error:  8.677112905190533e-08\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the implementation of SGD with Momentum\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D = 4, 5\n",
    "test_sgd = sequential(fc(N, D, name=\"sgd_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "\n",
    "test_sgd.layers[0].params = {\"sgd_fc_w\": w}\n",
    "test_sgd.layers[0].grads = {\"sgd_fc_w\": dw}\n",
    "\n",
    "\n",
    "test_sgd_wd = SGD(test_sgd, 1e-3, 1e-4)\n",
    "test_sgd_wd.step()\n",
    "\n",
    "updated_w = test_sgd.layers[0].params[\"sgd_fc_w\"]\n",
    "\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "       [-0.39936   , -0.34678632, -0.29421263, -0.24163895, -0.18906526],\n",
    "       [-0.13649158, -0.08391789, -0.03134421,  0.02122947,  0.07380316],\n",
    "       [ 0.12637684,  0.17895053,  0.23152421,  0.28409789,  0.33667158],\n",
    "       [ 0.38924526,  0.44181895,  0.49439263,  0.54696632,  0.59954   ]])\n",
    "\n",
    "\n",
    "print ('The following errors should be around or less than 1e-6')\n",
    "print ('updated_w error: ', rel_error(updated_w, expected_updated_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39936    -0.34678632 -0.29421263 -0.24163895 -0.18906526]\n",
      " [-0.13649158 -0.08391789 -0.03134421  0.02122947  0.07380316]\n",
      " [ 0.12637684  0.17895053  0.23152421  0.28409789  0.33667158]\n",
      " [ 0.38924526  0.44181895  0.49439263  0.54696632  0.59954   ]]\n"
     ]
    }
   ],
   "source": [
    "print(updated_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing SGD and SGD with Weight Decay [2pt]\n",
    "Run the following code block to train a multi-layer fully connected network with both SGD and SGD plus Weight Decay.\n",
    "You are expected to see Weight Decay have better validation accuracy than vinilla SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "# Arrange a small data\n",
    "num_train = 20000\n",
    "small_data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "reset_seed(seed=seed)\n",
    "model_sgd      = FullyConnectedNetwork()\n",
    "loss_f_sgd     = cross_entropy()\n",
    "optimizer_sgd  = SGD(model_sgd.net, 0.01)\n",
    "print (\"Training with Vanilla SGD...\")\n",
    "results_sgd = train_net(small_data_dict, model_sgd, loss_f_sgd, optimizer_sgd, batch_size=100, \n",
    "                        max_epochs=50, show_every=10000, verbose=True)\n",
    "\n",
    "import pickle\n",
    "with open('results_sgd.txt', 'wb') as f:\n",
    "    pickle.dump(results_sgd,f) \n",
    "    \n",
    "reset_seed(seed=seed)\n",
    "model_sgdw     = FullyConnectedNetwork()\n",
    "loss_f_sgdw    = cross_entropy()\n",
    "optimizer_sgdw = SGD(model_sgdw.net, 0.01, 1e-4)\n",
    "print (\"\\nTraining with SGD plus Weight Decay...\")\n",
    "results_sgdw = train_net(small_data_dict, model_sgdw, loss_f_sgdw, optimizer_sgdw, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=True)\n",
    "\n",
    "with open('results_sgdw.txt', 'wb') as f:\n",
    "    pickle.dump(results_sgdw,f)\n",
    "\n",
    "opt_params_sgd,  loss_hist_sgd,  train_acc_hist_sgd,  val_acc_hist_sgd  = results_sgd\n",
    "opt_params_sgdw, loss_hist_sgdw, train_acc_hist_sgdw, val_acc_hist_sgdw = results_sgdw\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Vanilla SGD...\n",
    "  0%|▍                                                                                 | 1/200 [00:12<41:06, 12.40s/it]\n",
    "(Iteration 1 / 10000) Average loss: 3.262084747551611\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:30<00:00,  4.95s/it]\n",
    "(Epoch 1 / 50) Training Accuracy: 0.13, Validation Accuracy: 0.13\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:21<00:00,  4.91s/it]\n",
    "(Epoch 2 / 50) Training Accuracy: 0.23, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:43<00:00,  4.72s/it]\n",
    "(Epoch 3 / 50) Training Accuracy: 0.16, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:29<00:00,  4.95s/it]\n",
    "(Epoch 4 / 50) Training Accuracy: 0.22, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:42<00:00,  4.41s/it]\n",
    "(Epoch 5 / 50) Training Accuracy: 0.28, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:14<00:00,  4.27s/it]\n",
    "(Epoch 6 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:51<00:00,  4.76s/it]\n",
    "(Epoch 7 / 50) Training Accuracy: 0.25, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:59<00:00,  4.50s/it]\n",
    "(Epoch 8 / 50) Training Accuracy: 0.23, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:11<00:00, 12.36s/it]\n",
    "(Epoch 9 / 50) Training Accuracy: 0.41, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:18<00:00, 12.39s/it]\n",
    "(Epoch 10 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:24<00:00, 12.42s/it]\n",
    "(Epoch 11 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.33\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:22<00:00, 12.41s/it]\n",
    "(Epoch 12 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [34:52<00:00, 10.46s/it]\n",
    "(Epoch 13 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [20:22<00:00,  6.11s/it]\n",
    "(Epoch 14 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [23:15<00:00,  6.98s/it]\n",
    "(Epoch 15 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [20:01<00:00,  6.01s/it]\n",
    "(Epoch 16 / 50) Training Accuracy: 0.32, Validation Accuracy: 0.21\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [19:40<00:00,  5.90s/it]\n",
    "(Epoch 17 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:41<00:00,  5.31s/it]\n",
    "(Epoch 18 / 50) Training Accuracy: 0.37, Validation Accuracy: 0.37\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:26<00:00,  4.63s/it]\n",
    "(Epoch 19 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:44<00:00,  4.72s/it]\n",
    "(Epoch 20 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.33\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:53<00:00,  5.37s/it]\n",
    "(Epoch 21 / 50) Training Accuracy: 0.34, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [22:01<00:00,  6.61s/it]\n",
    "(Epoch 22 / 50) Training Accuracy: 0.36, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [18:10<00:00,  5.45s/it]\n",
    "(Epoch 23 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:44<00:00,  5.32s/it]\n",
    "(Epoch 24 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [18:48<00:00,  5.64s/it]\n",
    "(Epoch 25 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [19:04<00:00,  5.72s/it]\n",
    "(Epoch 26 / 50) Training Accuracy: 0.29, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [18:27<00:00,  5.54s/it]\n",
    "(Epoch 27 / 50) Training Accuracy: 0.42, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:58<00:00,  4.79s/it]\n",
    "(Epoch 28 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:06<00:00,  4.83s/it]\n",
    "(Epoch 29 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:48<00:00,  5.34s/it]\n",
    "(Epoch 30 / 50) Training Accuracy: 0.46, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:32<00:00,  4.96s/it]\n",
    "(Epoch 31 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:04<00:00,  4.82s/it]\n",
    "(Epoch 32 / 50) Training Accuracy: 0.47, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:45<00:00,  4.73s/it]\n",
    "(Epoch 33 / 50) Training Accuracy: 0.45, Validation Accuracy: 0.2\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:55<00:00,  4.78s/it]\n",
    "(Epoch 34 / 50) Training Accuracy: 0.49, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:07<00:00,  5.14s/it]\n",
    "(Epoch 35 / 50) Training Accuracy: 0.48, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:38<00:00,  4.99s/it]\n",
    "(Epoch 36 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:07<00:00,  4.84s/it]\n",
    "(Epoch 37 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:59<00:00,  4.80s/it]\n",
    "(Epoch 38 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:50<00:00,  4.45s/it]\n",
    "(Epoch 39 / 50) Training Accuracy: 0.56, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:45<00:00,  4.43s/it]\n",
    "(Epoch 40 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 41 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:53<00:00,  4.47s/it]\n",
    "(Epoch 42 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:51<00:00,  4.46s/it]\n",
    "(Epoch 43 / 50) Training Accuracy: 0.58, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 44 / 50) Training Accuracy: 0.5, Validation Accuracy: 0.35\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:42<00:00,  4.41s/it]\n",
    "(Epoch 45 / 50) Training Accuracy: 0.6, Validation Accuracy: 0.35\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:46<00:00,  4.43s/it]\n",
    "(Epoch 46 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:42<00:00,  4.41s/it]\n",
    "(Epoch 47 / 50) Training Accuracy: 0.46, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 48 / 50) Training Accuracy: 0.59, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:46<00:00,  4.43s/it]\n",
    "(Epoch 49 / 50) Training Accuracy: 0.63, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 50 / 50) Training Accuracy: 0.51, Validation Accuracy: 0.21\n",
    "\n",
    "Training with SGD plus Weight Decay...\n",
    "  0%|                                                                                          | 0/200 [00:00<?, ?it/s]C:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\layer_utils.py:279: RuntimeWarning: overflow encountered in cosh\n",
    "  b = np.cosh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))\n",
    "  0%|▍                                                                                 | 1/200 [00:03<11:38,  3.51s/it]\n",
    "(Iteration 1 / 10000) Average loss: 3.262084747551611\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:12<00:00,  3.66s/it]\n",
    "(Epoch 1 / 50) Training Accuracy: 0.14, Validation Accuracy: 0.12\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:46<00:00,  4.13s/it]\n",
    "(Epoch 2 / 50) Training Accuracy: 0.23, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:23<00:00,  4.02s/it]\n",
    "(Epoch 3 / 50) Training Accuracy: 0.19, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:35<00:00,  4.08s/it]\n",
    "(Epoch 4 / 50) Training Accuracy: 0.22, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:04<00:00,  4.22s/it]\n",
    "(Epoch 5 / 50) Training Accuracy: 0.27, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:14<00:00,  4.27s/it]\n",
    "(Epoch 6 / 50) Training Accuracy: 0.27, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:08<00:00,  3.94s/it]\n",
    "(Epoch 7 / 50) Training Accuracy: 0.25, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:40<00:00,  3.80s/it]\n",
    "(Epoch 8 / 50) Training Accuracy: 0.26, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:38<00:00,  3.79s/it]\n",
    "(Epoch 9 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:37<00:00,  3.79s/it]\n",
    "(Epoch 10 / 50) Training Accuracy: 0.28, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:37<00:00,  3.79s/it]\n",
    "(Epoch 11 / 50) Training Accuracy: 0.27, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:38<00:00,  3.79s/it]\n",
    "(Epoch 12 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:32<00:00,  3.76s/it]\n",
    "(Epoch 13 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:22<00:00,  3.71s/it]\n",
    "(Epoch 14 / 50) Training Accuracy: 0.32, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:07<00:00,  3.94s/it]\n",
    "(Epoch 15 / 50) Training Accuracy: 0.29, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it]\n",
    "(Epoch 16 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 17 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 18 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:50<00:00,  4.45s/it]\n",
    "(Epoch 19 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:45<00:00,  4.43s/it]\n",
    "(Epoch 20 / 50) Training Accuracy: 0.38, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 21 / 50) Training Accuracy: 0.29, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:43<00:00,  4.42s/it]\n",
    "(Epoch 22 / 50) Training Accuracy: 0.34, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 23 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.38\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:42<00:00,  4.41s/it]\n",
    "(Epoch 24 / 50) Training Accuracy: 0.21, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it]\n",
    "(Epoch 25 / 50) Training Accuracy: 0.34, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 26 / 50) Training Accuracy: 0.24, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:40<00:00,  4.40s/it]\n",
    "(Epoch 27 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:43<00:00,  4.42s/it]\n",
    "(Epoch 28 / 50) Training Accuracy: 0.28, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:41<00:00,  4.41s/it]\n",
    "(Epoch 29 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.35\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:38<00:00,  4.39s/it]\n",
    "(Epoch 30 / 50) Training Accuracy: 0.32, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:39<00:00,  4.40s/it]\n",
    "(Epoch 31 / 50) Training Accuracy: 0.36, Validation Accuracy: 0.32\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:40<00:00,  4.40s/it]\n",
    "(Epoch 32 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.33\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [30:16<00:00,  9.08s/it]\n",
    "(Epoch 33 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [27:34<00:00,  8.27s/it]\n",
    "(Epoch 34 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it]\n",
    "(Epoch 35 / 50) Training Accuracy: 0.41, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it]\n",
    "(Epoch 36 / 50) Training Accuracy: 0.42, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 37 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:45<00:00,  4.43s/it]\n",
    "(Epoch 38 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.43\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 39 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.32\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it]\n",
    "(Epoch 40 / 50) Training Accuracy: 0.45, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:45<00:00,  4.43s/it]\n",
    "(Epoch 41 / 50) Training Accuracy: 0.34, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 42 / 50) Training Accuracy: 0.45, Validation Accuracy: 0.32\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 43 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:42<00:00,  4.41s/it]\n",
    "(Epoch 44 / 50) Training Accuracy: 0.47, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 45 / 50) Training Accuracy: 0.46, Validation Accuracy: 0.35\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 46 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.38\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:47<00:00,  4.44s/it]\n",
    "(Epoch 47 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:43<00:00,  4.42s/it]\n",
    "(Epoch 48 / 50) Training Accuracy: 0.36, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:43<00:00,  4.42s/it]\n",
    "(Epoch 49 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.33\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:44<00:00,  4.42s/it]\n",
    "(Epoch 50 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with L1 Regularization [2pts]\n",
    "With L1 Regularization, your regularized loss becomes $\\tilde{J}_\\mathrm{\\ell_1}(\\theta)$ and it's defined as\n",
    "$$\n",
    "\\tilde{J}_\\mathrm{\\ell_1}(\\theta) = J(\\theta) + \\lambda \\|\\theta\\|_{\\ell_1}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|\\theta\\|_{\\ell_1} = \\sum_{l=1}^n \\sum_{k=1}^{n_l} |\\theta_{l,k}|\n",
    "$$\n",
    "Please implmemt TODO block of ``apply_l1_regularization`` in ``lib/layer_utils``. Such regularization funcationality is called after gradient gathering in the ``backward`` process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange a small data\n",
    "num_train = 20000\n",
    "small_data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset_seed(seed=seed)\n",
    "model_sgd_l1     = FullyConnectedNetwork()\n",
    "loss_f_sgd_l1    = cross_entropy()\n",
    "optimizer_sgd_l1 = SGD(model_sgd_l1.net, 0.01)\n",
    "\n",
    "print (\"\\nTraining with SGD plus L1 Regularization...\")\n",
    "results_sgd_l1 = train_net(small_data_dict, model_sgd_l1, loss_f_sgd_l1, optimizer_sgd_l1, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=True, regularization=\"l1\", reg_lambda=1e-3)\n",
    "\n",
    "opt_params_sgd_l1, loss_hist_sgd_l1, train_acc_hist_sgd_l1, val_acc_hist_sgd_l1= results_sgd_l1\n",
    "\n",
    "import pickle\n",
    "with open('results_sgd_l1.txt', 'wb') as f:\n",
    "    pickle.dump(results_sgd_l1,f)\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l1, 'o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training with SGD plus L1 Regularization...\n",
    "  0%|                                                                                          | 0/200 [00:00<?, ?it/s]C:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\layer_utils.py:279: RuntimeWarning: overflow encountered in cosh\n",
    "  b = np.cosh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))\n",
    "  0%|▍                                                                                 | 1/200 [00:05<17:34,  5.30s/it]\n",
    "(Iteration 1 / 10000) Average loss: 3.262084747551611\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:19<00:00,  4.00s/it]\n",
    "(Epoch 1 / 50) Training Accuracy: 0.13, Validation Accuracy: 0.13\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:04<00:00,  3.62s/it]\n",
    "(Epoch 2 / 50) Training Accuracy: 0.23, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:00<00:00,  3.60s/it]\n",
    "(Epoch 3 / 50) Training Accuracy: 0.16, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:33<00:00,  4.07s/it]\n",
    "(Epoch 4 / 50) Training Accuracy: 0.22, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [12:32<00:00,  3.76s/it]\n",
    "(Epoch 5 / 50) Training Accuracy: 0.28, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [13:42<00:00,  4.11s/it]\n",
    "(Epoch 6 / 50) Training Accuracy: 0.3, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:03<00:00,  4.22s/it]\n",
    "(Epoch 7 / 50) Training Accuracy: 0.25, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [14:45<00:00,  4.43s/it]\n",
    "(Epoch 8 / 50) Training Accuracy: 0.23, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:10<00:00,  4.55s/it]\n",
    "(Epoch 9 / 50) Training Accuracy: 0.41, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:21<00:00,  4.61s/it]\n",
    "(Epoch 10 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.23\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:44<00:00,  4.72s/it]\n",
    "(Epoch 11 / 50) Training Accuracy: 0.31, Validation Accuracy: 0.33\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:33<00:00,  4.67s/it]\n",
    "(Epoch 12 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.22\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [16:02<00:00,  4.81s/it]\n",
    "(Epoch 13 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.26\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:41<00:00,  4.71s/it]\n",
    "(Epoch 14 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.29\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:47<00:00,  4.74s/it]\n",
    "(Epoch 15 / 50) Training Accuracy: 0.39, Validation Accuracy: 0.3\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:04<00:00,  4.52s/it]\n",
    "(Epoch 16 / 50) Training Accuracy: 0.32, Validation Accuracy: 0.21\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:10<00:00,  4.55s/it]\n",
    "(Epoch 17 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:20<00:00,  4.60s/it]\n",
    "(Epoch 18 / 50) Training Accuracy: 0.37, Validation Accuracy: 0.37\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:38<00:00,  4.69s/it]\n",
    "(Epoch 19 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:10<00:00,  4.55s/it]\n",
    "(Epoch 20 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.33\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:23<00:00,  4.62s/it]\n",
    "(Epoch 21 / 50) Training Accuracy: 0.34, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:25<00:00,  4.63s/it]\n",
    "(Epoch 22 / 50) Training Accuracy: 0.36, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [15:27<00:00,  4.64s/it]\n",
    "(Epoch 23 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [17:38<00:00,  5.29s/it]\n",
    "(Epoch 24 / 50) Training Accuracy: 0.33, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [17:09<00:00,  5.15s/it]\n",
    "(Epoch 25 / 50) Training Accuracy: 0.35, Validation Accuracy: 0.25\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [17:29<00:00,  5.25s/it]\n",
    "(Epoch 26 / 50) Training Accuracy: 0.29, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████| 200/200 [27:58<00:00,  8.39s/it]\n",
    "(Epoch 27 / 50) Training Accuracy: 0.42, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:01<00:00, 12.31s/it]\n",
    "(Epoch 28 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [40:53<00:00, 12.27s/it]\n",
    "(Epoch 29 / 50) Training Accuracy: 0.43, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:39<00:00, 12.50s/it]\n",
    "(Epoch 30 / 50) Training Accuracy: 0.46, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [40:45<00:00, 12.23s/it]\n",
    "(Epoch 31 / 50) Training Accuracy: 0.44, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [40:03<00:00, 12.02s/it]\n",
    "(Epoch 32 / 50) Training Accuracy: 0.47, Validation Accuracy: 0.31\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [23:10<00:00,  6.95s/it]\n",
    "(Epoch 33 / 50) Training Accuracy: 0.45, Validation Accuracy: 0.2\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [23:16<00:00,  6.98s/it]\n",
    "(Epoch 34 / 50) Training Accuracy: 0.49, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:20<00:00,  4.60s/it]\n",
    "(Epoch 35 / 50) Training Accuracy: 0.48, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:26<00:00,  4.63s/it]\n",
    "(Epoch 36 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.26\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:16<00:00,  4.58s/it]\n",
    "(Epoch 37 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.24\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:18<00:00,  4.59s/it]\n",
    "(Epoch 38 / 50) Training Accuracy: 0.4, Validation Accuracy: 0.34\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [27:09<00:00,  8.15s/it]\n",
    "(Epoch 39 / 50) Training Accuracy: 0.56, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:43<00:00, 12.52s/it]\n",
    "(Epoch 40 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:02<00:00, 12.31s/it]\n",
    "(Epoch 41 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.23\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [40:59<00:00, 12.30s/it]\n",
    "(Epoch 42 / 50) Training Accuracy: 0.54, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:07<00:00, 12.34s/it]\n",
    "(Epoch 43 / 50) Training Accuracy: 0.58, Validation Accuracy: 0.3\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:07<00:00, 12.34s/it]\n",
    "(Epoch 44 / 50) Training Accuracy: 0.5, Validation Accuracy: 0.35\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [41:05<00:00, 12.33s/it]\n",
    "(Epoch 45 / 50) Training Accuracy: 0.6, Validation Accuracy: 0.35\n",
    "100%|█████████████████████████████████████████████████████████████████████████████| 200/200 [5:55:05<00:00, 106.53s/it]\n",
    "(Epoch 46 / 50) Training Accuracy: 0.57, Validation Accuracy: 0.29\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:14<00:00,  4.87s/it]\n",
    "(Epoch 47 / 50) Training Accuracy: 0.46, Validation Accuracy: 0.22\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [16:55<00:00,  5.08s/it]\n",
    "(Epoch 48 / 50) Training Accuracy: 0.59, Validation Accuracy: 0.27\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [17:26<00:00,  5.23s/it]\n",
    "(Epoch 49 / 50) Training Accuracy: 0.63, Validation Accuracy: 0.28\n",
    "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [19:58<00:00,  5.99s/it]\n",
    "(Epoch 50 / 50) Training Accuracy: 0.51, Validation Accuracy: 0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with L2 Regularization [2pts]\n",
    "With L2 Regularization, your regularized loss becomes $\\tilde{J}_\\mathrm{\\ell_2}(\\theta)$ and it's defined as\n",
    "$$\n",
    "\\tilde{J}_\\mathrm{\\ell_2}(\\theta) = J(\\theta) + \\lambda \\|\\theta\\|_{\\ell_2}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|\\theta\\|_{\\ell_2} = \\sum_{l=1}^n \\sum_{k=1}^{n_l} \\theta_{l,k}^2\n",
    "$$\n",
    "Similarly, implmemt TODO block of ``apply_l2_regularization`` in ``lib/layer_utils``.\n",
    "For SGD, you're also asked to find the $\\lambda$ for L2 Regularization such that it achives the EXACTLY SAME effect as weight decay in the previous cells. As a reminder, learning rate is the same as previously, and the weight decay paramter was 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with SGD plus L2 Regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/200 [00:00<?, ?it/s]C:\\Users\\Sneha\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\layer_utils.py:279: RuntimeWarning: overflow encountered in cosh\n",
      "  b = np.cosh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))\n",
      " 62%|█████████████████████████████████████████████████▏                              | 123/200 [24:53<15:35, 12.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6472\\976296867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTraining with SGD plus L2 Regularization...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m results_sgd_l2 = train_net(small_data_dict, model_sgd_l2, loss_f_sgd_l2, optimizer_sgd_l2, batch_size=100, \n\u001b[0m\u001b[0;32m     13\u001b[0m                            max_epochs=50, show_every=10000, verbose=False, regularization=\"l2\", reg_lambda=l2_lambda)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\train.py\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(data, model, loss_func, optimizer, batch_size, max_epochs, lr_decay, lr_decay_every, show_every, verbose, regularization, reg_lambda)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m#Backward pass of the network : INCOMPLETE : REGULARIZATION INSTRUCTION READ ABOVE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mdLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mdX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;31m#Gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\fully_conn.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dprev, regularization, reg_lambda)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"none\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mdprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularization\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"l1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Jupyter Notebooks\\CSCI-566-Deep-Learning-and-its-Applications\\csci566-assignment1\\lib\\mlp\\layer_utils.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dprev)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.044715\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.044715\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mbinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reset_seed(seed=seed)\n",
    "model_sgd_l2     = FullyConnectedNetwork()\n",
    "loss_f_sgd_l2    = cross_entropy()\n",
    "optimizer_sgd_l2 = SGD(model_sgd_l2.net, 0.01)\n",
    "#####################################################################\n",
    "#### Find lambda for L2 regularization so that                   ####\n",
    "#### it achieves EXACTLY THE SAME learning curve as weight decay ####\n",
    "l2_lambda = None\n",
    "#####################################################################\n",
    "\n",
    "print (\"\\nTraining with SGD plus L2 Regularization...\")\n",
    "results_sgd_l2 = train_net(small_data_dict, model_sgd_l2, loss_f_sgd_l2, optimizer_sgd_l2, batch_size=100, \n",
    "                           max_epochs=50, show_every=10000, verbose=False, regularization=\"l2\", reg_lambda=l2_lambda)\n",
    "\n",
    "opt_params_sgd_l2, loss_hist_sgd_l2, train_acc_hist_sgd_l2, val_acc_hist_sgd_l2 = results_sgd_l2\n",
    "\n",
    "import pickle\n",
    "with open('results_sgd_l2.txt', 'wb') as f:\n",
    "    pickle.dump(results_sgd_l2,f)\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l1, 'o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l2, 'o', label=\"SGD with L2 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l2, '-o', label=\"SGD with L2 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l2, '-o', label=\"SGD with L2 Regularization\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam [2pt]\n",
    "The update rule of Adam is as shown below:  \n",
    "\\begin{align*}\n",
    "t &= t + 1 \\\\\n",
    "g_t &: \\text{gradients at update step } t \\\\\n",
    "m_t &= \\beta_1m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t &= \\beta_2v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n",
    "\\hat{m_t} &= m_t / (1 - \\beta_1^t) \\\\\n",
    "\\hat{v_t} &= v_t / (1 - \\beta_2^t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta\\ \\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon} \\\\\n",
    "\\end{align*}\n",
    "Complete the `Adam()` function in `lib/optim.py`\n",
    "Important Notes:\n",
    "1) $t$ must be updated before everything else\n",
    "2) $\\beta_1^t$ is $\\beta_1$ exponentiated to the $t$'th power\n",
    "3) You should also enable weight decay in Adam, similar to what you did in SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "N, D = 4, 5\n",
    "test_adam = sequential(fc(N, D, name=\"adam_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "test_adam.layers[0].params = {\"adam_fc_w\": w}\n",
    "test_adam.layers[0].grads = {\"adam_fc_w\": dw}\n",
    "\n",
    "opt_adam = Adam(test_adam, 1e-2, 0.9, 0.999, t=5)\n",
    "opt_adam.mt = {\"adam_fc_w\": m}\n",
    "opt_adam.vt = {\"adam_fc_w\": v}\n",
    "opt_adam.step()\n",
    "\n",
    "updated_w = test_adam.layers[0].params[\"adam_fc_w\"]\n",
    "mt = opt_adam.mt[\"adam_fc_w\"]\n",
    "vt = opt_adam.vt[\"adam_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print ('The following errors should be around or less than 1e-7')\n",
    "print ('updated_w error: ', rel_error(expected_updated_w, updated_w))\n",
    "print ('mt error: ', rel_error(expected_m, mt))\n",
    "print ('vt error: ', rel_error(expected_v, vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_w,'\\n\\n',vt,'\\n\\n',mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Weight Decay v.s. L2 Regularization in Adam [5pt]\n",
    "Run the following code block to compare the plotted results between effects of weight decay and L2 regularization on Adam. Are they still the same? (we can make them the same as in SGD, can we also do it in Adam?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "reset_seed(seed)\n",
    "model_adam_wd      = FullyConnectedNetwork()\n",
    "loss_f_adam_wd     = cross_entropy()\n",
    "optimizer_adam_wd  = Adam(model_adam_wd.net, lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "print (\"Training with AdamW...\")\n",
    "results_adam_wd = train_net(small_data_dict, model_adam_wd, loss_f_adam_wd, optimizer_adam_wd, batch_size=100, \n",
    "                        max_epochs=50, show_every=10000, verbose=False)\n",
    "\n",
    "import pickle\n",
    "with open('results_adam_wd.txt', 'wb') as f:\n",
    "    pickle.dump(results_adam_wd,f)\n",
    "\n",
    "reset_seed(seed)\n",
    "model_adam_l2      = FullyConnectedNetwork()\n",
    "loss_f_adam_l2     = cross_entropy()\n",
    "optimizer_adam_l2 = Adam(model_adam_l2.net, lr=1e-4)\n",
    "reg_lambda_l2 = 1e-4\n",
    "print (\"\\nTraining with Adam + L2...\")\n",
    "results_adam_l2 = train_net(small_data_dict, model_adam_l2, loss_f_adam_l2, optimizer_adam_l2, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=False, regularization='l2', reg_lambda=reg_lambda_l2)\n",
    "\n",
    "import pickle\n",
    "with open('results_adam_l2.txt', 'wb') as f:\n",
    "    pickle.dump(results_adam_l2,f)\n",
    "    \n",
    "opt_params_adam_wd, loss_hist_adam_wd, train_acc_hist_adam_wd, val_acc_hist_adam_wd  = results_adam_wd\n",
    "opt_params_adam_l2, loss_hist_adam_l2, train_acc_hist_adam_l2, val_acc_hist_adam_l2 = results_adam_l2\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam_wd, 'o', label=\"Adam with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam_wd, '-o', label=\"Adam with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam_wd, '-o', label=\"Adam with Weight Decay\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam_l2, 'o', label=\"Adam with L2\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam_l2, '-o', label=\"Adam with L2\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam_l2, '-o', label=\"Adam with L2\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_796\\2310822204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresults_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results_sgdw.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mresults_sgdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnbconvert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLExporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('results_sgd_l1.txt', 'rb') as f:\n",
    "    results_sgd_l1 = pickle.load(f)\n",
    "with open('results_sgd.txt', 'rb') as f:\n",
    "    results_sgd = pickle.load(f)\n",
    "with open('results_sgdw.txt', 'rb') as f:\n",
    "    results_sgdw = pickle.load(f)\n",
    "with open('results_sgd_l2.txt', 'rb') as f:\n",
    "    results_sgd_l2 = pickle.load(f)\n",
    "with open('results_adam_wd.txt', 'rb') as f:\n",
    "    results_adam_wd = pickle.load(f)\n",
    "with open('results_adam_l2.txt', 'rb') as f:\n",
    "    results_adam_l2 = pickle.load(f)\n",
    "    \n",
    "from nbconvert import HTMLExporter\n",
    "import codecs\n",
    "import nbformat\n",
    "\n",
    "notebook_name = 'Problem_1.ipynb'\n",
    "output_file_name = 'output_prob_1.html'\n",
    "\n",
    "exporter = HTMLExporter()\n",
    "output_notebook = nbformat.read(notebook_name, as_version=4)\n",
    "\n",
    "output, resources = exporter.from_notebook_node(output_notebook)\n",
    "codecs.open(output_file_name, 'w', encoding='utf-8').write(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_1_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for the simple neural network training with > 30% validation accuracy\n",
    "2. Plots for comparing vanilla SGD to SGD + Weight Decay, SGD + L1 and SGD + L2\n",
    "3. \"Comparing different Regularizations\" plots\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
